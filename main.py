import streamlit as st
import os
import time
import google.generativeai as genai
from dotenv import load_dotenv
import cv2
import pandas as pd
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain.callbacks import StreamlitCallbackHandler
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
import json
from crewai import Agent, Task, Crew
import base64

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

# ë¹„ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
def upload_and_process_file(file_path):
    print(f"Uploading file: {file_path}...")
    video_file = genai.upload_file(path=file_path)
    
    # íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ í™•ì¸
    while video_file.state.name == "PROCESSING":
        print('.', end='', flush=True)
        time.sleep(10)
        video_file = genai.get_file(video_file.name)
    
    if video_file.state.name == "FAILED":
        raise ValueError(f"File processing failed: {video_file.state.name}")
    
    print(f"\nCompleted upload: {video_file.uri}")
    return video_file

# LLM ìš”ì²­ í•¨ìˆ˜
def generate_content_from_video(video_file, prompt, model_name="gemini-1.5-flash-001", timeout=600):
    print("Making LLM inference request...")
    model = genai.GenerativeModel(model_name=model_name)
    response = model.generate_content([video_file, prompt], request_options={"timeout": timeout})
    return response

def extract_video_segment(input_video, start_time, end_time, output_folder, food_name):
    cap = cv2.VideoCapture(input_video)
    
    # ë¹„ë””ì˜¤ ì†ì„± ê°€ì ¸ì˜¤ê¸°
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    
    # ì‹œì‘ ë° ì¢…ë£Œ í”„ë ˆì„ ê³„ì‚°
    start_frame = int(start_time * fps)
    end_frame = int(end_time * fps)
    
    # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'H264')  # ë˜ëŠ” 'avc1', 'H264' ë“±ì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    output_filename = f"{food_name}_{start_time}_{end_time}.mp4"
    output_path = os.path.join(output_folder, output_filename)
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    if not out.isOpened():
        print(f"Could not open output video file: {output_path}")
        return None
    
    # ì‹œì‘ í”„ë ˆì„ìœ¼ë¡œ ì´ë™
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    # í”„ë ˆì„ ì¶”ì¶œ ë° ì €ì¥
    for frame_num in range(start_frame, end_frame):
        ret, frame = cap.read()
        if not ret:
            print(f"Failed to read frame {frame_num}")
            break
        out.write(frame)
    
    # ë¦¬ì†ŒìŠ¤ í•´ì œ
    cap.release()
    out.release()
    
    if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
        print(f"Successfully extracted video segment: {output_path}")
        return output_path
    else:
        print(f"Failed to extract video segment or output file is empty: {output_path}")
        return None

def create_metadata_table(extracted_videos, food_name):
    metadata = []
    for video in extracted_videos:
        metadata.append({
            'video_path': video,
            'food_name': food_name
        })
    
    df = pd.DataFrame(metadata)
    df.to_csv('metadata.csv', index=False)
    print("ë©”íƒ€ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return df

# Pydantic ëª¨ë¸
class Seconds(BaseModel):
    start: int = Field(description="ìŒì‹ ì‹œì‘ ì‹œê°„(ì´ˆ)")
    end: int = Field(description="ìŒì‹ ì¢…ë£Œ ì‹œê°„(ì´ˆ)")

class VideoParser(BaseModel):
    time: list[Seconds] = Field(description="ìŒì‹ ì‹œê°„(ì´ˆ)")
    food_name: list[str] = Field(description="ìŒì‹ ì´ë¦„")

model = ChatOpenAI(api_key=OPENAI_API_KEY, model="gpt-3.5-turbo", temperature=0)

@st.cache_resource
def process_video_and_create_qa(file_path):
    video_file = upload_and_process_file(file_path)
    
    prompt = '''
    í•´ë‹¹ ì˜ìƒì—ì„œ ë‹¹ì‹ ì€ ë¨¹ëŠ” ì‹œê°„ì˜ ì‹œì‘ê³¼ ëì„ ì¶œë ¥í•˜ê³ , ë¨¹ëŠ” ë©”ë‰´ë„ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ì–´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
    [ì¶œë ¥ì˜ˆì‹œ]
    [1:01,1:31], bulgogi
    [2:01,2:31], bibimbab
    [3:01,3:31], kimchijjigae
    '''
    
    response = generate_content_from_video(video_file, prompt)
    
    parser = JsonOutputParser(pydantic_object=VideoParser)
    prompt_template = PromptTemplate(
        template="ì‚¬ìš©ì ì¿¼ë¦¬ì— ë‹µí•˜ì„¸ìš”.\n{format_instructions}\n{query}\n",
        input_variables=["query"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
    )
    chain = prompt_template | model | parser
    parsed_response = chain.invoke({"query": response.text})
    
    output_folder = "extract_video"
    os.makedirs(output_folder, exist_ok=True)
    extracted_videos = []
    
    # ê° ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•´ ë¹„ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ê³  ê° ì„¸ê·¸ë¨¼íŠ¸ì— ë§ëŠ” ìŒì‹ ì´ë¦„ì„ ì ìš©
    for idx, segment in enumerate(parsed_response['time']):
        # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ìŒì‹ ì´ë¦„ì„ ì¸ë±ìŠ¤ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        food_name = parsed_response['food_name'][idx]  # ìŒì‹ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©
        extracted_path = extract_video_segment(file_path, segment['start'], segment['end'], output_folder, food_name)
        extracted_videos.append(extracted_path)
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± (ìŒì‹ ì´ë¦„ì´ ë¦¬ìŠ¤íŠ¸ë¡œ ì œëŒ€ë¡œ ë„˜ì–´ê°€ë„ë¡ ìˆ˜ì •)
    create_metadata_table(extracted_videos, parsed_response['food_name'])

    
    loader = CSVLoader(file_path='metadata.csv', encoding='utf-8')
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    docs = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(docs, embeddings)

    food_names = parsed_response['food_name']
    print(food_names)

    system_prompt = f"""ë‹¹ì‹ ì€ ìŒì‹ ì˜ìƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
    contextì—ëŠ” ìœ ì €ê°€ ì§ˆë¬¸í•œ ìŒì‹ì— ê´€í•œ ì •ë³´ê°€ ìˆìŠµë‹ˆë‹¤. 
    ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì •ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³  ê°€ëŠ¥í•œ ê²½ìš° ê´€ë ¨ëœ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
    ì¶œë ¥ì€ ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ìŒì‹ì— ë§ëŠ” ë§›ìˆëŠ” í‘œí˜„ì„ ìƒê°í•´ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    
    ì˜ìƒì—ì„œ í™•ì¸ëœ ìŒì‹ ëª©ë¡: {', '.join(food_names)}"""
    
    qa_prompt = PromptTemplate(
        template=system_prompt + "\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:",
        input_variables=["context", "question"]
    )

    qa = RetrievalQA.from_chain_type(
        llm=model, 
        chain_type="stuff", 
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": qa_prompt}
    )
    
    return qa


def find_relevant_video(query, vectorstore):
    # ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
    docs = vectorstore.similarity_search(query, k=1)
    print("ê´€ë ¨ìˆëŠ” ë¬¸ì„œ: ", docs)  # ë¬¸ì„œê°€ ê²€ìƒ‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
    
    if docs:
        # ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œ page_content ì¶”ì¶œ
        content = docs[0].page_content
        print("ë¬¸ì„œì˜ ë‚´ìš©: ", content)  # page_content ë‚´ìš© í™•ì¸
        
        # video_pathì™€ food_nameì„ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ íŒŒì‹±
        lines = content.split('\n')
        video_path = None
        food_name = None
        
        for line in lines:
            if line.startswith('video_path:'):
                video_path = line.split('video_path: ')[1].strip()
            elif line.startswith('food_name:'):
                food_name = line.split('food_name: ')[1].strip()
        
        print("ì¶”ì¶œëœ video_path: ", video_path)
        print("ì¶”ì¶œëœ food_name: ", food_name)
        
        return video_path, food_name
    
    return None, None

# Streamlit ì•±
def main():
    st.title("ìŒì‹ ì˜ìƒ ë¶„ì„ ë° ì±„íŒ… ì•±")

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "ë™ì˜ìƒì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}]
    
    if "qa" not in st.session_state:
        st.session_state["qa"] = None
    
    if "vectorstore" not in st.session_state:
        st.session_state["vectorstore"] = None
    
    if "extracted_videos" not in st.session_state:
        st.session_state["extracted_videos"] = []

    # ì‚¬ì´ë“œë°”ì— íŒŒì¼ ì—…ë¡œë” ì¶”ê°€
    uploaded_file = st.sidebar.file_uploader("ë™ì˜ìƒ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["mp4"])

    if uploaded_file and st.session_state["qa"] is None:
        with st.spinner("ë™ì˜ìƒì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with open("temp_video.mp4", "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            st.session_state["qa"] = process_video_and_create_qa("temp_video.mp4")
            
            # ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
            loader = CSVLoader(file_path='metadata.csv', encoding='utf-8')
            documents = loader.load()
            embeddings = OpenAIEmbeddings()
            st.session_state["vectorstore"] = Chroma.from_documents(documents, embeddings)
            
            st.session_state.messages.append({"role": "assistant", "content": "ì˜ìƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸í•´ì£¼ì„¸ìš”."})

    for msg in st.session_state.messages:
        st.chat_message(msg["role"], avatar="ğŸ§‘â€ğŸ’»" if msg["role"] == "user" else "ğŸ¤–").write(msg["content"])

    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").write(prompt)

        if st.session_state["qa"] and st.session_state["vectorstore"]:
            # Crew ìƒì„± ë° ì‹¤í–‰
            research_agent = Agent(
                role='Video Recommender',
                goal='Determine if the user wants to watch a video based on the query',
                backstory="""You are an AI agent responsible for analyzing the user's query
                and deciding whether or not they want to watch a video.""",
                verbose=True
            )
            task = Task(
                description=f'{prompt}ì— ëŒ€í•˜ì—¬ ë™ì˜ìƒì„ í‹€ì§€ ë§ì§€ ê²°ì •í•˜ëŠ” ì‘ì—…',
                expected_output='0 if the user does not want to watch a video, 1 if the user wants to watch a video',
                agent=research_agent,
            )
            crew = Crew(
                agents=[research_agent],
                tasks=[task],
                verbose=True
            )
            result = crew.kickoff(inputs=dict(query=prompt))
            
            show_video = int(result.raw) == 1

            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st_callback = StreamlitCallbackHandler(st.container())
                response_placeholder = st.empty()
                full_response = ""

                for chunk in st.session_state["qa"].stream({"query": prompt}, callbacks=[st_callback]):
                    if isinstance(chunk, dict) and 'result' in chunk:
                        full_response += chunk['result']
                        response_placeholder.markdown(full_response + "â–Œ")
                
                response_placeholder.markdown(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})

            if show_video:
                # ì¿¼ë¦¬ì— ê¸°ë°˜í•˜ì—¬ ê´€ë ¨ ë¹„ë””ì˜¤ ì°¾ê¸°
                video_path, food_name = find_relevant_video(prompt, st.session_state["vectorstore"])
                
                if video_path:
                    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    absolute_path = os.path.abspath(video_path)
                    print("ë¹„ë””ì˜¤ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ:", absolute_path)
                    
                    # ë¹„ë””ì˜¤ ì¶œë ¥ (í¬ê¸° ì¡°ì ˆ ì—†ì´)
                    st.video(absolute_path)
                    st.write(f"ì¬ìƒ ì¤‘ì¸ ìŒì‹: {food_name}")
                    
                    # íŒŒì¼ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if os.path.exists(absolute_path):
                        print("ë¹„ë””ì˜¤ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        print("ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ë¹„ë””ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.session_state.messages.append({"role": "assistant", "content": "ë¨¼ì € ë™ì˜ìƒì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."})
            st.chat_message("assistant", avatar="ğŸ¤–").write("ë¨¼ì € ë™ì˜ìƒì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
