{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일에서 환경 변수를 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1725510286.817403 1842701 config.cc:230] gRPC experiments enabled: call_status_override_on_cancellation, event_engine_dns, event_engine_listener, http2_stats_fix, monitoring_experiment, pick_first_new, trace_record_callops, work_serializer_clears_time_cache\n",
      "I0000 00:00:1725510286.823818 1842701 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file: video/내가 엘든링 7번 깬 비법.f614.mp4...\n",
      ".\n",
      "Completed upload: https://generativelanguage.googleapis.com/v1beta/files/blh6evvnwwcg\n",
      "Making LLM inference request...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725510302.115880 1842701 check_gcp_environment_no_op.cc:29] ALTS: Platforms other than Linux and Windows are not supported\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "[0:08, 0:57], 딸기 크림 빙수"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# .env에서 API 키 가져오기\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Google API key not found. Please set it in your .env file.\")\n",
    "\n",
    "# API 키 설정\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 비디오 파일 경로\n",
    "\n",
    "\n",
    "# 파일 업로드 함수\n",
    "def upload_and_process_file(file_path):\n",
    "    print(f\"Uploading file: {file_path}...\")\n",
    "    video_file = genai.upload_file(path=file_path)\n",
    "    \n",
    "    # 파일 처리 상태 확인\n",
    "    while video_file.state.name == \"PROCESSING\":\n",
    "        print('.', end='', flush=True)\n",
    "        time.sleep(10)\n",
    "        video_file = genai.get_file(video_file.name)\n",
    "    \n",
    "    if video_file.state.name == \"FAILED\":\n",
    "        raise ValueError(f\"File processing failed: {video_file.state.name}\")\n",
    "    \n",
    "    print(f\"\\nCompleted upload: {video_file.uri}\")\n",
    "    return video_file\n",
    "\n",
    "# LLM 요청 함수\n",
    "def generate_content_from_video(video_file, prompt, model_name=\"gemini-1.5-flash-001\", timeout=600):\n",
    "    print(\"Making LLM inference request...\")\n",
    "    model = genai.GenerativeModel(model_name=model_name)\n",
    "    response = model.generate_content([video_file, prompt], request_options={\"timeout\": timeout})\n",
    "    return response\n",
    "\n",
    "# 메인 로직\n",
    "if __name__ == \"__main__\":\n",
    "    video_file_name = \"video/내가 엘든링 7번 깬 비법.f614.mp4\"\n",
    "    video_file = upload_and_process_file(video_file_name)\n",
    "    # 프롬프트 설정\n",
    "    prompt = '''\n",
    "    해당영상에서 당신은 먹는 시간의 시작과 끝을 출력하고, 먹는 메뉴도 출력해야합니다.\n",
    "    [출력예시]\n",
    "    [1:01,1:31], 불고기\n",
    "    [2:01,2:31], 비빔밥\n",
    "    [3:01,3:31], 김치찌개\n",
    "    '''\n",
    "    \n",
    "    # 컨텐츠 생성 요청\n",
    "    response = generate_content_from_video(video_file, prompt)\n",
    "    \n",
    "    # 마크다운으로 출력\n",
    "    display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0:08, 0:57], 딸기 크림 빙수'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': [{'start': 8, 'end': 57}], 'food_name': '딸기 크림 빙수'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY ,model=\"gpt-4o-mini\",temperature=0)\n",
    "\n",
    "\n",
    "class Seconds(BaseModel):\n",
    "    start: int = Field(description=\"The start time of the food in seconds\")\n",
    "    end: int = Field(description=\"The end time of the food in seconds\")\n",
    "\n",
    "class VideoParser(BaseModel):\n",
    "    time: list[Seconds] = Field(description=\"The time of the food in seconds\")\n",
    "    food_name: str = Field(description=\"The name of the food\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=VideoParser)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\"query\": response.text})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 8, 'end': 57}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출된 비디오: extract_video/딸기 크림 빙수_8_57.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def extract_video_segment(input_video, start_time, end_time, output_folder, food_name):\n",
    "    # 입력 비디오 열기\n",
    "    cap = cv2.VideoCapture(input_video)\n",
    "    \n",
    "    # 비디오 속성 가져오기\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # 출력 비디오 설정\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    output_filename = f\"{food_name}_{start_time}_{end_time}.mp4\"\n",
    "    output_path = os.path.join(output_folder, output_filename)\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # 시작 프레임으로 이동\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_time * fps)\n",
    "    \n",
    "    # 프레임 추출 및 저장\n",
    "    for _ in range((end_time - start_time) * fps):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "    \n",
    "    # 리소스 해제\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# 사용 예시\n",
    "input_video = \"video/내가 엘든링 7번 깬 비법.f614.mp4\"\n",
    "output_folder = \"extract_video\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for segment in response['time']:\n",
    "    extracted_path = extract_video_segment(input_video, segment['start'], segment['end'], output_folder, response['food_name'])\n",
    "    print(f\"추출된 비디오: {extracted_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "메타데이터가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_metadata_table(extracted_videos, food_name):\n",
    "    metadata = []\n",
    "    for video in extracted_videos:\n",
    "        metadata.append({\n",
    "            'video_path': video,\n",
    "            'food_name': food_name\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.to_csv('metadata.csv', index=False)\n",
    "    print(\"메타데이터가 저장되었습니다.\")\n",
    "    return df\n",
    "\n",
    "# 사용 예시\n",
    "extracted_videos = [f for f in os.listdir(output_folder) if f.endswith('.mp4')]\n",
    "metadata_df = create_metadata_table([os.path.join(output_folder, v) for v in extracted_videos], response['food_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_path</th>\n",
       "      <th>food_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>extract_video/딸기 크림 빙수_8_57.mp4</td>\n",
       "      <td>딸기 크림 빙수</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        video_path food_name\n",
       "0  extract_video/딸기 크림 빙수_8_57.mp4  딸기 크림 빙수"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '어떤 음식이 영상에 나오나요?', 'result': ' 계란 술안주 and 딸기 크림 빙수'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# CSV 파일 로드\n",
    "loader = CSVLoader(file_path='metadata.csv', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "# 텍스트 분할\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 임베딩 및 벡터 저장소 생성\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embeddings)\n",
    "\n",
    "# 검색 기반 QA 체인 생성\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "# 사용 예시\n",
    "query = \"어떤 음식이 영상에 나오나요?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'metadata.csv', 'row': 0}, page_content='video_path: extract_video/딸기 크림 빙수_8_57.mp4\\nfood_name: 딸기 크림 빙수')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725515991.438615 1842701 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting routellm[eval,serve]\n",
      "  Downloading routellm-0.2.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (6.0.1)\n",
      "Requirement already satisfied: pydantic in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (2.8.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (2.2.2)\n",
      "Collecting torch (from routellm[eval,serve])\n",
      "  Downloading torch-2.4.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting scikit-learn (from routellm[eval,serve])\n",
      "  Using cached scikit_learn-1.5.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (4.66.4)\n",
      "Requirement already satisfied: openai in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (1.37.1)\n",
      "Collecting transformers (from routellm[eval,serve])\n",
      "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets (from routellm[eval,serve])\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting litellm (from routellm[eval,serve])\n",
      "  Downloading litellm-1.44.17-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting matplotlib (from routellm[eval,serve])\n",
      "  Using cached matplotlib-3.9.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting pandarallel (from routellm[eval,serve])\n",
      "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sglang (from routellm[eval,serve])\n",
      "  Downloading sglang-0.3.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (0.7.0)\n",
      "Requirement already satisfied: fastapi in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (0.112.0)\n",
      "Collecting shortuuid (from routellm[eval,serve])\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: uvicorn in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from routellm[eval,serve]) (0.30.5)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from datasets->routellm[eval,serve]) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from datasets->routellm[eval,serve]) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->routellm[eval,serve])\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from datasets->routellm[eval,serve]) (2.32.3)\n",
      "Collecting xxhash (from datasets->routellm[eval,serve])\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->routellm[eval,serve])\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->routellm[eval,serve]) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from datasets->routellm[eval,serve]) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from datasets->routellm[eval,serve]) (0.24.6)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from datasets->routellm[eval,serve]) (24.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from fastapi->routellm[eval,serve]) (0.37.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from fastapi->routellm[eval,serve]) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from pydantic->routellm[eval,serve]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from pydantic->routellm[eval,serve]) (2.20.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from litellm->routellm[eval,serve]) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from litellm->routellm[eval,serve]) (8.2.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from litellm->routellm[eval,serve]) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from litellm->routellm[eval,serve]) (4.23.0)\n",
      "Collecting openai (from routellm[eval,serve])\n",
      "  Using cached openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from litellm->routellm[eval,serve]) (1.0.1)\n",
      "Requirement already satisfied: tokenizers in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from litellm->routellm[eval,serve]) (0.20.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from openai->routellm[eval,serve]) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from openai->routellm[eval,serve]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from openai->routellm[eval,serve]) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from openai->routellm[eval,serve]) (0.4.2)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from openai->routellm[eval,serve]) (1.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from tiktoken->routellm[eval,serve]) (2023.12.25)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->routellm[eval,serve])\n",
      "  Downloading contourpy-1.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->routellm[eval,serve])\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->routellm[eval,serve])\n",
      "  Using cached fonttools-4.53.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->routellm[eval,serve])\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from matplotlib->routellm[eval,serve]) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from matplotlib->routellm[eval,serve]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from matplotlib->routellm[eval,serve]) (2.9.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from pandarallel->routellm[eval,serve]) (5.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from pandas->routellm[eval,serve]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from pandas->routellm[eval,serve]) (2024.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->routellm[eval,serve])\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->routellm[eval,serve])\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->routellm[eval,serve])\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from torch->routellm[eval,serve]) (1.13.2)\n",
      "Collecting networkx (from torch->routellm[eval,serve])\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->routellm[eval,serve])\n",
      "  Using cached safetensors-0.4.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers (from litellm->routellm[eval,serve])\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from uvicorn->routellm[eval,serve]) (0.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->routellm[eval,serve]) (3.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from aiohttp->datasets->routellm[eval,serve]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from aiohttp->datasets->routellm[eval,serve]) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from aiohttp->datasets->routellm[eval,serve]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from aiohttp->datasets->routellm[eval,serve]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from aiohttp->datasets->routellm[eval,serve]) (1.9.4)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->routellm[eval,serve]) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->routellm[eval,serve]) (1.0.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm->routellm[eval,serve]) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from jinja2<4.0.0,>=3.1.2->litellm->routellm[eval,serve]) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->routellm[eval,serve]) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->routellm[eval,serve]) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->routellm[eval,serve]) (0.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->routellm[eval,serve]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from requests>=2.32.2->datasets->routellm[eval,serve]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from requests>=2.32.2->datasets->routellm[eval,serve]) (2.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/gemini/lib/python3.11/site-packages (from sympy->torch->routellm[eval,serve]) (1.3.0)\n",
      "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading litellm-1.44.17-py3-none-any.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached openai-1.43.0-py3-none-any.whl (365 kB)\n",
      "Using cached matplotlib-3.9.2-cp311-cp311-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Downloading routellm-0.2.0-py3-none-any.whl (50 kB)\n",
      "Using cached scikit_learn-1.5.1-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Downloading sglang-0.3.0-py3-none-any.whl (326 kB)\n",
      "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Downloading torch-2.4.1-cp311-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "Downloading contourpy-1.3.0-cp311-cp311-macosx_11_0_arm64.whl (250 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fonttools-4.53.1-cp311-cp311-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading kiwisolver-1.4.7-cp311-cp311-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached safetensors-0.4.4-cp311-cp311-macosx_11_0_arm64.whl (381 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Building wheels for collected packages: pandarallel\n",
      "  Building wheel for pandarallel (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16673 sha256=b96d6ffa076678a710558476483b16f8d002b6f81e8453e96fb3dd197b6996b2\n",
      "  Stored in directory: /Users/kdb/Library/Caches/pip/wheels/b9/c6/5a/829298789e94348b81af52ab42c19d49da007306bbcc983827\n",
      "Successfully built pandarallel\n",
      "Installing collected packages: xxhash, threadpoolctl, shortuuid, scipy, safetensors, networkx, kiwisolver, joblib, fonttools, dill, cycler, contourpy, torch, sglang, scikit-learn, multiprocess, matplotlib, tokenizers, pandarallel, openai, transformers, litellm, datasets, routellm\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.0\n",
      "    Uninstalling tokenizers-0.20.0:\n",
      "      Successfully uninstalled tokenizers-0.20.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.37.1\n",
      "    Uninstalling openai-1.37.1:\n",
      "      Successfully uninstalled openai-1.37.1\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 datasets-2.21.0 dill-0.3.8 fonttools-4.53.1 joblib-1.4.2 kiwisolver-1.4.7 litellm-1.44.17 matplotlib-3.9.2 multiprocess-0.70.16 networkx-3.3 openai-1.43.0 pandarallel-1.6.5 routellm-0.2.0 safetensors-0.4.4 scikit-learn-1.5.1 scipy-1.14.1 sglang-0.3.0 shortuuid-1.0.13 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.4.1 transformers-4.44.2 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"routellm[serve,eval]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725516046.017926 1842701 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RouteLLM'...\n",
      "remote: Enumerating objects: 1027, done.\u001b[K\n",
      "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
      "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
      "remote: Total 1027 (delta 117), reused 122 (delta 110), pack-reused 861 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1027/1027), 5.36 MiB | 9.67 MiB/s, done.\n",
      "Resolving deltas: 100% (626/626), done.\n",
      "/Users/kdb/Desktop/rag_study/RouteLLM\n",
      "zsh:1: no matches found: .[serve,eval]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725516047.909814 1842701 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lm-sys/RouteLLM.git\n",
    "%cd RouteLLM\n",
    "!pip install -e .[serve,eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': [{'start': 8, 'end': 57}], 'food_name': '딸기 크림 빙수'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "model = ChatOpenAI(api_key=OPENAI_API_KEY ,model=\"gpt-4o-mini\",temperature=0)\n",
    "\n",
    "\n",
    "class Seconds(BaseModel):\n",
    "    start: int = Field(description=\"The start time of the food in seconds\")\n",
    "    end: int = Field(description=\"The end time of the food in seconds\")\n",
    "\n",
    "class VideoParser(BaseModel):\n",
    "    time: list[Seconds] = Field(description=\"The time of the food in seconds\")\n",
    "    food_name: str = Field(description=\"The name of the food\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=VideoParser)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\"query\": response.text})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: .[serve,eval]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725516368.047000 1842701 work_stealing_thread_pool.cc:320] WorkStealingThreadPoolImpl::PrepareFork\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e .[serve,eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"아 그래 ? 나는 백종원이 먹는 동영상을 보고싶은데\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_MODEL_NAME\"] = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "\n",
    "# Research Agent 생성\n",
    "research_agent = Agent(\n",
    "    role='Video Recommender',\n",
    "    goal='Determine if the user wants to watch a video based on the query',\n",
    "    backstory=\"\"\"You are an AI agent responsible for analyzing the user's query\n",
    "    and deciding whether or not they want to watch a video.\"\"\",  # 사용할 LLM 지정\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 사용자 쿼리\n",
    "user_input = \"아 그래 ? 나는 백종원이 먹는 동영상을 보고싶은데\"\n",
    "\n",
    "# Task 정의: 동영상 추천 여부 판단\n",
    "task = Task(\n",
    "    description='Analyze the user input and decide if a video should be played',\n",
    "    expected_output='0 if the user does not want to watch a video, 1 if the user wants to watch a video',\n",
    "    agent=research_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew 생성\n",
    "crew = Crew(\n",
    "    agents=[research_agent],\n",
    "    tasks=[task],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Crew 실행\n",
    "result = crew.kickoff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
